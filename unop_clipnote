F25
def senti_mc():
    iv_chg_min=15
    pct_c_min=20
    pct_c_max=100- pct_c_min
    dm=read_sql("SELECT * FROM tbl_mc_raw")
    dm['iv_chg']=dm['iv_chg'].str.replace('%','')
    dm['iv_chg']=dm['iv_chg'].astype(float)  
    dm['p_chg']=dm['p_chg'].str.replace('%','')
    dm['p_chg']=dm['p_chg'].astype(float)
    
    dmg=dm.groupby('date')
    data=[]
    for n,g in dmg:
        data.append([n,g[g.pct_c>pct_c_max].shape[0]/g.shape[0], g[g.pct_c<pct_c_min].shape[0]/g.shape[0]])
    data=np.asarray(data)
    data=data.transpose()
    cols=['date','pct_c','pct_p']
    df=pd.DataFrame(dict(zip(cols,data)))
        
    df['pct_c']=df['pct_c'].astype(float)
    df['pct_p']=df['pct_p'].astype(float)
    df['pct_cp']=df['pct_c']/df['pct_p']
    df['c-p']=df['pct_c']-df['pct_p']
    
    dp=read_sql("SELECT * FROM tbl_pv_etf WHERE ticker='SPY'")
#    df.plot(x='date', y=['pct_c', 'pct_p'], kind='line', rot=90)
    df.plot(x='date', y=['pct_c', 'pct_p'], kind='line', rot=90)
    

F21:
unop_mc: convert iv to float
unop_bc/ tbl_candies: dc['iv'].str.replace("%","").astype(float)
$323.62 x 31.6% x SQRT (22/365) = $25.11

p_range_22d: d1['mv']=d1['p']*d1['iv']/100*np.sqrt(20/252)

* plot df_stat (p_22d, dt, iv, hv)  axv_horizontal line (iv, p_22d)
ax.annotate('First maxima', 
            xy=(p_22d, x_lim.max()), 
            xytext=(np.pi/2., 2.3),rrowprops=dict(facecolor='black', shrink=0.05)

def sec_leadlag(q_date, sec=''):
    ds=read_sql("SELECT * FROM tbl_stat")
    ds['date']=pd.to_datetime(ds['date']).dt.date
    period=22
    p_date=q_date - datetime.timedelta(period)
    ds=ds[ds.sec==sec.upper()]
    ds.sort_values(['date','rtn_22_pct'], inplace=True)
    ds=ds[ds.date>=p_date]
    lead=ds[ds.date==q_date].tail(5)
    lead['momt']='lead'
    lag=ds[ds.date==q_date].head(5)
    lag['momt']='lag'
    tag=pd.concat([lead,lag], axis=0)
    for t in tag['ticker']:
        ds.loc[ds.ticker==t,'rtn_5_pct_avg']=ds[ds.ticker==t]['rtn_5_pct'].mean()
    ds_avg=ds[['ticker','rtn_5_pct_avg']]    
    ds_avg.drop_duplicates(keep='first', inplace=True)
    dm=tag.merge(ds_avg, on='ticker', how='left')
    dm.loc[dm.momt=='lead', 'agree']=dm[dm.momt=='lead']['rtn_5_pct']>dm[dm.momt=='lead']['rtn_5_pct_avg']
    dm.loc[dm.momt=='lag', 'agree']=dm[dm.momt=='lag']['rtn_5_pct']<dm[dm.momt=='lag']['rtn_5_pct_avg']
    show=['ticker','sec','momt','rtn_22_pct','agree', 'rtn_5_pct','rtn_5_pct_avg']
    print (dm[show])    
        
    
def sec_stat():
    ds=read_sql("SELECT * FROM tbl_stat_etf")
    dsg=ds.groupby('ticker')
    data=[]
    for n, g in dsg:
        period=22
        g.sort_values('date', inplace=True)
        g=g.tail(period)
        std_22='{:.4f}'.format(g['sartn_22'].std())
        max_22=g['rtn_22_pct'].max()
        min_22=g['rtn_22_pct'].min()
        begin_22=g['sartn_22'].head(1).values[0].astype(float)
        end_22=g['sartn_22'].tail(1).values[0].astype(float)
        avg_5='{:.1f}'.format(g['rtn_5_pct'].mean())
 #       ratio=(float(end_22)-float(begin_22))/std_5
        data_tmp=[n,std_22, begin_22, end_22, max_22, min_22, avg_5]
        data.append(data_tmp)
    data=np.asarray(data)
    data=data.transpose()
    cols=['ticker','std_22','begin_22','end_22', 'max_22','min_22', 'avg_5']
    df=pd.DataFrame(dict(zip(cols, data)))
    df['end_22']=df['end_22'].astype(float)
    df['chg_22']=(df['end_22'].astype(float) - df['begin_22'].astype(float))
    df['sharpe']=df['chg_22']/df['std_22'].astype(float)

    df.sort_values(['sharpe', 'chg_22'], ascending=(True, False), inplace=True)
    print(df)
    return df



eoddate backup source:
https://pypi.org/project/eoddata-client/#history
https://medium.com/python-data/quandl-getting-end-of-day-stock-data-with-python-8652671d6661
http://www.csidata.com/factsheets.php?type=stock&format=html&exchangeid=AMEX
https://product.intrinio.com/financial-data/us-stock-prices-api/pricing
FRED:  https://research.stlouisfed.org/docs/api/fred/
CMR: https://coherentlogic.com/middleware-development/cmr/

Feb9
1. stat_view label on y-axis
https://stackoverflow.com/questions/49237522/annotate-end-of-lines-using-python-and-matplotlib
2. etf risk gauge
https://etfdb.com/etf-education/understanding-etf-s-total-cost-of-ownership/
https://seekingalpha.com/article/760061-how-to-gauge-risk-appetite?page=2
3. var
https://medium.com/quaintitative/measuring-market-risk-in-python-658d022c2b70
4. risk parameter
https://fred.stlouisfed.org/series/BAMLH0A0HYM2
https://seekingalpha.com/article/4004394-risk-appetite-indicator-keep-eye-financial-markets?page=2
5. FRED
https://fred.stlouisfed.org/categories/32425
6. sp fwd pe
https://ycharts.com/indicators/sandp_500_pe_ratio_forward_estimate



jan.21@hm
1. review R-stat 
2. data strucutre
3. play strategy c1-10 table to see validate/ back testing
4. 

Jan.22
@ Priority:
0. flask input( spec/candy/trd ->enforce data validty check: dt, 
   data_check(): flask input, web status ready, before daily routine
   flow: 
   data_check(), backup_db()
   daily: 	spec_candy(s_list)-> @spec_edit 
			@trd_edit -> track_raw
		
   if trd_entry:  trd_candy(t_list)-> @candy_edit ->trd_entry()
  

1. intel - iv_rank, ear_dt, leads: repeat, stk_vol move, sec rotate/top/botom, LCS

2. track - simple/all ast
3. spike profile plot/quantify
4. Flask - table, 
5. bc_etf

@ candy_action:
. bc _etf/inx 
6. Flask_ track view (easier)
7. earn_dt, div_dt ("re.search('(\d{1,2}-\d{2}-\d{4})',e1).group(0)")

@ spike:
1. plot_all (earn_dt, div_dt, ma, opt_vol, stk_vol, iv, hv)
2. hv_20/60 overlay: agree? trend, mark iv_30
3. spike_7 day (or cust)
4. spike_quantify: well behave, 2_std_window?
5. hv/iv overlay: hv pattern? hv to predict iv valuation->decide play/expire
(mc.com iv 1 year chart)

MAIN
1. FIX - get_earning_ec
2. FIX - get_op (IV rank, chart)
2. Data integrity - Alchemy
3. FLASK - display table
4. Intel - Spike/ HV profile -> pattern
5. Vol arb - IV map HV / HV predict
6. Auto - dnld/ Cloud
7. new info: twit/news/ <avoid surprise>
8. insider
9. fundamental: leverage info.
10. barometer: fx/etf
11.ideas: BB monthy?

@track_raw():
Fix get _op to get iv rank


Resource:
https://pandas.pydata.org/pandas-docs/stable/cookbook.html
MAIN
1. FIX - get_earning_ec
2. FIX - get_op (IV rank, chart)
2. Data integrity - Alchemy
3. FLASK - display table
4. Intel - Spike/ HV profile -> pattern
5. Vol arb - IV map HV / HV predict
6. Auto - dnld/ Cloud

@home
1. trd_to_candy (also from tbl_spec_candy
2. candy_to_trd (flask template: candy_main update to tbl_c? append)
3. intel (bc/mc -> filter high p, c, or p/c coexit, remove sweep b/s logic)


a. add field "way","pc" to tbl_c
b. update pc,way, lsn, note in tbl_c via flask
c. update R-track.py
d. fix R-track bug
e. update tgt_dt, tgt_p (per spike_profile check)
f. R-track: con_exit_no_change (tgt_dt passed spike freq expected)
g. spike check for LP: TRGP, GPS, CMCSA (catalyst, spike profile)

mc , bc (call %, ingore last price? so b/s does not matter)


TASK:
0. FIX intel-lame
1. track_raw() - error cap, str float
2. candy_template fix
3. mc_intel, untel (too many lame)
4. spike_profile
5. bc _etf/inx 
6. Flask_ track view (easier)
7. earn_dt, div_dt (substring fix)

MAIN
1. FIX - get_earning_ec
2. FIX - get_op (IV rank, chart)
2. Data integrity - Alchemy
3. FLASK - display table
4. Intel - Spike/ HV profile -> pattern
5. Vol arb - IV map HV / HV predict
6. Auto - dnld/ Cloud

Resource:
https://pandas.pydata.org/pandas-docs/stable/cookbook.html

@copy db daily
from shutil import copyfile
db_source=r"C:\Users\qli1\BNS_wspace\flask\f_trd.db"
db_dest=r"C:\Users\qli1\BNS_wspace\flask\f_trd_bkup.db"
copyfile(db_source, db_dest)

@webpage ready:
import requests
urls=["https://marketchameleon.com/Reports/UnusualOptionVolumeReport", ]
def url_ok(urls):
	for url in urls:
		r = requests.head(url)
		print(url, r.status_code == 200)

@track
a=da.groupby('act')['pct'].sum()
b=da.groupby('act')['value'].sum()
z=list(zip(a,b)
pd.DataFrame(z,columns=['pct','value'],index=a.index)
